{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Generating Natural Language Summaries from Factsheets"
      ],
      "metadata": {
        "id": "JTe0DvxhD26w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setup"
      ],
      "metadata": {
        "id": "d9T1uKypDvhU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FXT_mRWCj3_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b14c6c5-4f99-424a-935d-321432a070f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import pandas\n",
        "\n",
        "import re\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.utils.capture import capture_output"
      ],
      "metadata": {
        "id": "c5OTyh9Olj2O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "%cd /content/drive/MyDrive/CS 159 Project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1QYBPyomydv",
        "outputId": "d506f44f-2ab0-4fdf-ff9d-d1197faf00c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1LeLbrNZayyv5i4FVOL44lq8EKgwzSuZl/CS 159 Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###OpenAI Key"
      ],
      "metadata": {
        "id": "eAHBREdDZTz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipython = get_ipython() # run generated code\n",
        "# YOUR_KEY = os.getenv(\"sk-proj-W5F0FE87iGYN9VS679VeT3BlbkFJDAjPPplBSgT4gqd5IPMZ\")\n",
        "YOUR_KEY = \"sk-proj-n5GwGeTfcKLlcFi8qAyrT3BlbkFJKcYBL0VwMgaEVJowisbT\"\n",
        "\n",
        "client = OpenAI(api_key = YOUR_KEY)\n",
        "\n",
        "pattern = re.compile(r'```python\\n(.*?)```', re.DOTALL)  # extract code from llm generation"
      ],
      "metadata": {
        "id": "g9cV2QeyloCe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "fysnYtQ5RhHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def load_preprocess():\n",
        "  # Load the dataset\n",
        "  multi_lexsum = load_dataset(\"allenai/multi_lexsum\", name=\"v20220616\")\n",
        "  train = multi_lexsum[\"test\"] # The first instance of the dev set\n",
        "  raw_data_train = []\n",
        "  summaries_train = {\"long\": [], \"short\": [], \"tiny\": []}\n",
        "  i = 0\n",
        "  DEBUG = False\n",
        "\n",
        "  for case1 in train:\n",
        "    # each case has 4-5 sources\n",
        "    if DEBUG and i == 10:\n",
        "      break\n",
        "    raw_data_train.append(case1[\"sources\"])\n",
        "\n",
        "    for sum_len in [\"long\", \"short\", \"tiny\"]:\n",
        "      summaries_train[sum_len].append(case1[\"summary/\" + sum_len])\n",
        "    if DEBUG:\n",
        "      i += 1\n",
        "  return raw_data_train, summaries_train"
      ],
      "metadata": {
        "id": "3vxj87mXRkAZ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Natural Language Summaries from CSVs/Worksheets"
      ],
      "metadata": {
        "id": "8BtTvEqaFx3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from evaluate import load\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def process_csv(csv_file_path):\n",
        "  csv_data = []\n",
        "  with open(csv_file_path, newline='') as csvfile:\n",
        "      csvreader = csv.reader(csvfile)\n",
        "      for row in csvreader:\n",
        "          csv_data.append(row)\n",
        "  return csv_data\n",
        "\n",
        "\n",
        "def generate_string(csv_data):\n",
        "  csv_string = ''\n",
        "  for i in range(len(csv_data)):\n",
        "    if i == 0:\n",
        "        continue\n",
        "    else:\n",
        "      csv_string += ': '.join(csv_data[i]) + '\\n'\n",
        "  return csv_string\n",
        "\n",
        "\n",
        "\n",
        "def generate_summary(processed_string):\n",
        "    NUM_WORDS = 130\n",
        "    template = '''\n",
        "    Setting: You are a helpful and concise assistant designed to assist users in summarizing factsheets.\n",
        "\n",
        "    Provide clear and precise answers to the user's questions. Avoid unnecessary details and keep your responses brief and to the point.\n",
        "\n",
        "    Your goal is to understand the user's request, and provide text to\n",
        "    fulfill the request.\n",
        "\n",
        "    Your input will be a factsheet with the following format -->\n",
        "    category1 : detail1\n",
        "    category2 : detail2\n",
        "    ...\n",
        "\n",
        "    Your output will be text.\n",
        "    '''\n",
        "\n",
        "    prompt_create = f'''\n",
        "    You are a lawyer describing the court case to the general public.\n",
        "    Given the factsheet in the following text format -->\n",
        "    category1 : detail1\n",
        "    category2 : detail2\n",
        "    ...\n",
        "\n",
        "    Summarize the factsheet so that it is understable to the general public.\n",
        "    The summary should be paragraph form around {NUM_WORDS} words. Below is the factsheet:\n",
        "    {processed_string}\n",
        "\n",
        "     '''\n",
        "    # Prompting till we get 10 summaries w 650 words\n",
        "    try_times = 0\n",
        "    chat_summaries_short = []\n",
        "    TOTAL_TRIES = 10\n",
        "    while try_times < TOTAL_TRIES:\n",
        "      summary = get_openai_response(prompt_create, template)\n",
        "      sum_words = len(summary.split(\" \"))\n",
        "      if sum_words > NUM_WORDS - 20 and sum_words < NUM_WORDS + 20:\n",
        "        try_times += 1\n",
        "        chat_summaries_short.append(summary)\n",
        "\n",
        "    return chat_summaries_short"
      ],
      "metadata": {
        "id": "inKJOzMt1Og1"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrics"
      ],
      "metadata": {
        "id": "j6YQypWSC9p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exact_match(pred, ground_truth):\n",
        "    if len(pred) < len(ground_truth):\n",
        "        ground_truth = ground_truth[:len(pred)]\n",
        "    elif len(pred) > len(ground_truth):\n",
        "        pred = pred[:len(ground_truth)]\n",
        "    exact_match = load(\"exact_match\")\n",
        "    results = exact_match.compute(references=pred, predictions=ground_truth)\n",
        "    return round(results[\"exact_match\"], 2)\n",
        "\n",
        "\n",
        "def evaluate(chat_summaries, ground_truth):\n",
        "  exact_match_scores = []\n",
        "  for summary in chat_summaries:\n",
        "    exact_match_scores.append(exact_match(summary, ground_truth))\n",
        "  return exact_match_scores\n",
        "\n",
        "def find_best_summary(exact_match_scores):\n",
        "  best_index = exact_match_scores.index(max(exact_match_scores))\n",
        "  return best_index\n",
        "\n",
        "\n",
        "def compute_cosine_similarity(chat_summaries, ground_truth, verbose=True):\n",
        "  \"\"\"\n",
        "  Compute the cosine similarity between the source and summary sheets.\n",
        "  source_sheet: string representation of the source sheet\n",
        "  summary_sheet: string representation of the summary sheet\n",
        "  \"\"\"\n",
        "  cosine_scores = []\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  for summary in chat_summaries:\n",
        "    tfidf_matrix = vectorizer.fit_transform([summary, ground_truth])\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "    cosine_scores.append(cosine_sim[0][0])\n",
        "  return cosine_scores"
      ],
      "metadata": {
        "id": "LLLOJHk318jp"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Loop"
      ],
      "metadata": {
        "id": "Fl-ZXr2kDaki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main(fact_sheet_path):\n",
        "  raw_data_train, summaries_train = load_preprocess()\n",
        "  # change short vs long depending on factsheet type / size\n",
        "  # specify correct index\n",
        "  ground_truth = summaries_train['short'][0]\n",
        "\n",
        "  processed_csv = generate_string(process_csv(fact_sheet_path))\n",
        "  chat_summaries_short = generate_summary(processed_csv)\n",
        "  exact_match_scores = evaluate(chat_summaries_short, ground_truth)\n",
        "  cosine_sim_scores = compute_cosine_similarity(chat_summaries_short, ground_truth)\n",
        "  best_idx = find_best_summary(cosine_sim_scores)\n",
        "\n",
        "  print(\"Exact similarity scores: \", exact_match_scores)\n",
        "  print(\"Cosine similarity scores: \", cosine_sim_scores)\n",
        "  print(\"Here is the best summary: \", chat_summaries_short[int(best_idx)])\n",
        "\n"
      ],
      "metadata": {
        "id": "235RTy705gNi"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fact_sheet_path = 'factsheet/CJ-AL-0020_summary.csv'\n",
        "main(fact_sheet_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "---ZjtCJB7Yt",
        "outputId": "f1b17175-f359-4bf2-e429-dc9a85c6d8ea"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact similarity scores:  [0.06, 0.06, 0.05, 0.07, 0.07, 0.07, 0.07, 0.05, 0.08, 0.05]\n",
            "Cosine similarity scores:  [0.4538061785234016, 0.41453297094674457, 0.4479310312073994, 0.45540559761938754, 0.4144960736739172, 0.4770789469498656, 0.4834145675095602, 0.4974333667609191, 0.35644706061362696, 0.453882105174925]\n",
            "Here is the best summary:  In August 2013, an indigent detainee in Montgomery Municipal Jail filed a lawsuit in the Circuit Court of Montgomery County, Alabama against the City of Montgomery and Judge Westry. The lawsuit, based on federal law, alleged violations of constitutional rights. The detainee, facing fines from traffic tickets, was arrested and given a choice by Judge Westry to pay or serve jail time without legal representation. The case moved to federal court, underwent discovery and mediation, leading to a settlement in 2014. The settlement included attorney fees and established procedures for indigent defendants. The final judgment in November 2014 affirmed the constitutionality of the agreed procedures. The case, represented by the Southern Poverty Law Center, concluded after the settlement agreement was reached.\n"
          ]
        }
      ]
    }
  ]
}